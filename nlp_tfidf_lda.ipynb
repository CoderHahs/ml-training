{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hrith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(1110)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "**RedditNews.csv** contains Daily News headlines from the Stock Market.\n",
    "This data set can be found [here](https://www.kaggle.com/aaron7sun/stocknews/data#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Datasets/stocknews/RedditNews.csv', error_bad_lines=False);\n",
    "data_text = data[['News']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "We will perform the following steps:\n",
    "- **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "- Words that have fewer than 3 characters are removed.\n",
    "- All **stopwords** are removed.\n",
    "- Words are **lemmatized** — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "- Words are **stemmed** — words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "[\"Delhi's\", 'air', 'quality', 'improves', 'after', '9', 'days', 'of', 'odd-even', 'formula,', 'pollution', 'levels', 'drop', 'sharply']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['delhi', 'qualiti', 'improv', 'day', 'formula', 'pollut', 'level', 'drop', 'sharpli']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [year, woman, mexico, citi, final, receiv, bir...\n",
       "1            [chief, back, athen, perman, olymp, host]\n",
       "2          [presid, franc, say, brexit, donald, trump]\n",
       "3    [british, polic, hour, notic, threaten, hunger...\n",
       "4    [nobel, laureat, urg, greenpeac, stop, oppos, ...\n",
       "5    [brazil, huge, spike, number, polic, kill, ahe...\n",
       "6    [austria, highest, court, annul, presidenti, e...\n",
       "7    [facebook, win, privaci, case, track, belgian,...\n",
       "8    [switzerland, deni, muslim, girl, citizenship,...\n",
       "9    [china, kill, million, innoc, medit, organ, re...\n",
       "Name: News, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['News'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words on the Dataset\n",
    "*gensim.corpora.Dictionary(processed_docs)* creates a dictionary that stores the frequency of each word\n",
    "\n",
    "*dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=1000)* filters out words that have less than 15 occurences and keeps the top 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 alvarez\n",
      "1 bear\n",
      "2 birth\n",
      "3 certif\n",
      "4 citi\n",
      "5 die\n",
      "6 final\n",
      "7 hour\n",
      "8 later\n",
      "9 lira\n",
      "10 mexico\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(115, 1), (332, 1), (517, 1), (688, 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LDA with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.072*\"israel\" + 0.050*\"iran\" + 0.035*\"gaza\" + 0.030*\"protest\" + 0.030*\"isra\" + 0.027*\"nuclear\" + 0.021*\"palestinian\" + 0.018*\"power\" + 0.015*\"say\" + 0.012*\"minist\"\n",
      "Topic: 1 \n",
      "Words: 0.027*\"drug\" + 0.019*\"muslim\" + 0.019*\"mexico\" + 0.014*\"franc\" + 0.013*\"school\" + 0.013*\"court\" + 0.013*\"children\" + 0.012*\"say\" + 0.011*\"legal\" + 0.011*\"islam\"\n",
      "Topic: 2 \n",
      "Words: 0.040*\"year\" + 0.031*\"death\" + 0.020*\"prison\" + 0.019*\"kill\" + 0.019*\"iraq\" + 0.017*\"elect\" + 0.016*\"woman\" + 0.015*\"jail\" + 0.015*\"women\" + 0.015*\"murder\"\n",
      "Topic: 3 \n",
      "Words: 0.036*\"russian\" + 0.035*\"russia\" + 0.025*\"troop\" + 0.019*\"afghanistan\" + 0.018*\"syria\" + 0.017*\"presid\" + 0.017*\"militari\" + 0.017*\"georgia\" + 0.016*\"ship\" + 0.014*\"say\"\n",
      "Topic: 4 \n",
      "Words: 0.020*\"germani\" + 0.015*\"dead\" + 0.015*\"german\" + 0.013*\"abus\" + 0.011*\"rise\" + 0.011*\"earthquak\" + 0.011*\"turn\" + 0.011*\"look\" + 0.010*\"near\" + 0.010*\"photo\"\n",
      "Topic: 5 \n",
      "Words: 0.052*\"attack\" + 0.047*\"kill\" + 0.031*\"pakistan\" + 0.031*\"isra\" + 0.026*\"arrest\" + 0.025*\"polic\" + 0.023*\"bomb\" + 0.017*\"terror\" + 0.015*\"palestinian\" + 0.015*\"shoot\"\n",
      "Topic: 6 \n",
      "Words: 0.044*\"north\" + 0.043*\"korea\" + 0.040*\"south\" + 0.026*\"unit\" + 0.025*\"nation\" + 0.025*\"state\" + 0.020*\"human\" + 0.019*\"africa\" + 0.018*\"canada\" + 0.017*\"right\"\n",
      "Topic: 7 \n",
      "Words: 0.068*\"china\" + 0.022*\"world\" + 0.019*\"parti\" + 0.012*\"govern\" + 0.012*\"worker\" + 0.012*\"iceland\" + 0.011*\"polit\" + 0.010*\"say\" + 0.009*\"chines\" + 0.008*\"burn\"\n",
      "Topic: 8 \n",
      "Words: 0.045*\"world\" + 0.022*\"year\" + 0.018*\"time\" + 0.016*\"india\" + 0.013*\"minist\" + 0.013*\"million\" + 0.012*\"food\" + 0.011*\"bank\" + 0.011*\"peopl\" + 0.011*\"get\"\n",
      "Topic: 9 \n",
      "Words: 0.032*\"polic\" + 0.019*\"secret\" + 0.016*\"forc\" + 0.015*\"govern\" + 0.014*\"chines\" + 0.013*\"european\" + 0.012*\"right\" + 0.010*\"internet\" + 0.010*\"media\" + 0.010*\"report\"\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "# for i in range(len(bow_doc_4310)):\n",
    "#     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], dictionary[bow_doc_4310[i][0]], bow_doc_4310[i][1]))\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of LDA with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.010*\"china\" + 0.009*\"news\" + 0.009*\"ahmadinejad\" + 0.009*\"dead\" + 0.009*\"ship\" + 0.008*\"protest\" + 0.008*\"think\" + 0.008*\"train\" + 0.008*\"internet\" + 0.007*\"egypt\"\n",
      "Topic: 1 Word: 0.013*\"gaza\" + 0.012*\"china\" + 0.010*\"water\" + 0.009*\"world\" + 0.009*\"school\" + 0.008*\"billion\" + 0.008*\"israel\" + 0.008*\"die\" + 0.008*\"million\" + 0.008*\"popul\"\n",
      "Topic: 2 Word: 0.020*\"israel\" + 0.020*\"palestinian\" + 0.019*\"olymp\" + 0.019*\"isra\" + 0.010*\"taliban\" + 0.009*\"arab\" + 0.008*\"jewish\" + 0.007*\"win\" + 0.007*\"game\" + 0.007*\"gaza\"\n",
      "Topic: 3 Word: 0.018*\"georgia\" + 0.016*\"troop\" + 0.015*\"bush\" + 0.015*\"russia\" + 0.013*\"zimbabw\" + 0.010*\"iraq\" + 0.009*\"iraqi\" + 0.009*\"trade\" + 0.008*\"greec\" + 0.008*\"crisi\"\n",
      "Topic: 4 Word: 0.011*\"saudi\" + 0.011*\"world\" + 0.010*\"abus\" + 0.010*\"women\" + 0.010*\"minist\" + 0.009*\"child\" + 0.008*\"face\" + 0.008*\"prime\" + 0.008*\"year\" + 0.007*\"church\"\n",
      "Topic: 5 Word: 0.015*\"human\" + 0.015*\"right\" + 0.010*\"wikileak\" + 0.009*\"crimin\" + 0.008*\"european\" + 0.008*\"assang\" + 0.007*\"countri\" + 0.007*\"australia\" + 0.007*\"say\" + 0.007*\"judg\"\n",
      "Topic: 6 Word: 0.026*\"kill\" + 0.016*\"attack\" + 0.014*\"death\" + 0.014*\"korea\" + 0.013*\"pakistan\" + 0.012*\"north\" + 0.012*\"bomb\" + 0.011*\"year\" + 0.009*\"prison\" + 0.009*\"shoot\"\n",
      "Topic: 7 Word: 0.018*\"drug\" + 0.013*\"mexico\" + 0.011*\"price\" + 0.011*\"india\" + 0.008*\"earthquak\" + 0.008*\"pope\" + 0.007*\"food\" + 0.007*\"miss\" + 0.007*\"world\" + 0.007*\"china\"\n",
      "Topic: 8 Word: 0.014*\"elect\" + 0.013*\"russian\" + 0.011*\"putin\" + 0.011*\"presid\" + 0.011*\"say\" + 0.010*\"russia\" + 0.009*\"militari\" + 0.008*\"chavez\" + 0.008*\"vote\" + 0.008*\"parliament\"\n",
      "Topic: 9 Word: 0.026*\"nuclear\" + 0.021*\"iran\" + 0.013*\"pirat\" + 0.012*\"israel\" + 0.011*\"japan\" + 0.009*\"missil\" + 0.008*\"plant\" + 0.008*\"dubai\" + 0.008*\"fire\" + 0.008*\"pictur\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delhi',\n",
       " 'qualiti',\n",
       " 'improv',\n",
       " 'day',\n",
       " 'formula',\n",
       " 'pollut',\n",
       " 'level',\n",
       " 'drop',\n",
       " 'sharpli']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8199676871299744\t \n",
      "Topic: 0.045*\"world\" + 0.022*\"year\" + 0.018*\"time\" + 0.016*\"india\" + 0.013*\"minist\" + 0.013*\"million\" + 0.012*\"food\" + 0.011*\"bank\" + 0.011*\"peopl\" + 0.011*\"get\"\n",
      "\n",
      "Score: 0.020009731873869896\t \n",
      "Topic: 0.068*\"china\" + 0.022*\"world\" + 0.019*\"parti\" + 0.012*\"govern\" + 0.012*\"worker\" + 0.012*\"iceland\" + 0.011*\"polit\" + 0.010*\"say\" + 0.009*\"chines\" + 0.008*\"burn\"\n",
      "\n",
      "Score: 0.020006822422146797\t \n",
      "Topic: 0.020*\"germani\" + 0.015*\"dead\" + 0.015*\"german\" + 0.013*\"abus\" + 0.011*\"rise\" + 0.011*\"earthquak\" + 0.011*\"turn\" + 0.011*\"look\" + 0.010*\"near\" + 0.010*\"photo\"\n",
      "\n",
      "Score: 0.020003536716103554\t \n",
      "Topic: 0.027*\"drug\" + 0.019*\"muslim\" + 0.019*\"mexico\" + 0.014*\"franc\" + 0.013*\"school\" + 0.013*\"court\" + 0.013*\"children\" + 0.012*\"say\" + 0.011*\"legal\" + 0.011*\"islam\"\n",
      "\n",
      "Score: 0.02000279910862446\t \n",
      "Topic: 0.040*\"year\" + 0.031*\"death\" + 0.020*\"prison\" + 0.019*\"kill\" + 0.019*\"iraq\" + 0.017*\"elect\" + 0.016*\"woman\" + 0.015*\"jail\" + 0.015*\"women\" + 0.015*\"murder\"\n",
      "\n",
      "Score: 0.0200020931661129\t \n",
      "Topic: 0.036*\"russian\" + 0.035*\"russia\" + 0.025*\"troop\" + 0.019*\"afghanistan\" + 0.018*\"syria\" + 0.017*\"presid\" + 0.017*\"militari\" + 0.017*\"georgia\" + 0.016*\"ship\" + 0.014*\"say\"\n",
      "\n",
      "Score: 0.020002076402306557\t \n",
      "Topic: 0.072*\"israel\" + 0.050*\"iran\" + 0.035*\"gaza\" + 0.030*\"protest\" + 0.030*\"isra\" + 0.027*\"nuclear\" + 0.021*\"palestinian\" + 0.018*\"power\" + 0.015*\"say\" + 0.012*\"minist\"\n",
      "\n",
      "Score: 0.02000194974243641\t \n",
      "Topic: 0.044*\"north\" + 0.043*\"korea\" + 0.040*\"south\" + 0.026*\"unit\" + 0.025*\"nation\" + 0.025*\"state\" + 0.020*\"human\" + 0.019*\"africa\" + 0.018*\"canada\" + 0.017*\"right\"\n",
      "\n",
      "Score: 0.02000182494521141\t \n",
      "Topic: 0.052*\"attack\" + 0.047*\"kill\" + 0.031*\"pakistan\" + 0.031*\"isra\" + 0.026*\"arrest\" + 0.025*\"polic\" + 0.023*\"bomb\" + 0.017*\"terror\" + 0.015*\"palestinian\" + 0.015*\"shoot\"\n",
      "\n",
      "Score: 0.0200014878064394\t \n",
      "Topic: 0.032*\"polic\" + 0.019*\"secret\" + 0.016*\"forc\" + 0.015*\"govern\" + 0.014*\"chines\" + 0.013*\"european\" + 0.012*\"right\" + 0.010*\"internet\" + 0.010*\"media\" + 0.010*\"report\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8199640512466431\t \n",
      "Topic: 0.013*\"gaza\" + 0.012*\"china\" + 0.010*\"water\" + 0.009*\"world\" + 0.009*\"school\" + 0.008*\"billion\" + 0.008*\"israel\" + 0.008*\"die\" + 0.008*\"million\" + 0.008*\"popul\"\n",
      "\n",
      "Score: 0.02000817097723484\t \n",
      "Topic: 0.026*\"kill\" + 0.016*\"attack\" + 0.014*\"death\" + 0.014*\"korea\" + 0.013*\"pakistan\" + 0.012*\"north\" + 0.012*\"bomb\" + 0.011*\"year\" + 0.009*\"prison\" + 0.009*\"shoot\"\n",
      "\n",
      "Score: 0.02000444382429123\t \n",
      "Topic: 0.010*\"china\" + 0.009*\"news\" + 0.009*\"ahmadinejad\" + 0.009*\"dead\" + 0.009*\"ship\" + 0.008*\"protest\" + 0.008*\"think\" + 0.008*\"train\" + 0.008*\"internet\" + 0.007*\"egypt\"\n",
      "\n",
      "Score: 0.020004086196422577\t \n",
      "Topic: 0.018*\"drug\" + 0.013*\"mexico\" + 0.011*\"price\" + 0.011*\"india\" + 0.008*\"earthquak\" + 0.008*\"pope\" + 0.007*\"food\" + 0.007*\"miss\" + 0.007*\"world\" + 0.007*\"china\"\n",
      "\n",
      "Score: 0.020003946498036385\t \n",
      "Topic: 0.011*\"saudi\" + 0.011*\"world\" + 0.010*\"abus\" + 0.010*\"women\" + 0.010*\"minist\" + 0.009*\"child\" + 0.008*\"face\" + 0.008*\"prime\" + 0.008*\"year\" + 0.007*\"church\"\n",
      "\n",
      "Score: 0.020003415644168854\t \n",
      "Topic: 0.020*\"israel\" + 0.020*\"palestinian\" + 0.019*\"olymp\" + 0.019*\"isra\" + 0.010*\"taliban\" + 0.009*\"arab\" + 0.008*\"jewish\" + 0.007*\"win\" + 0.007*\"game\" + 0.007*\"gaza\"\n",
      "\n",
      "Score: 0.020003311336040497\t \n",
      "Topic: 0.018*\"georgia\" + 0.016*\"troop\" + 0.015*\"bush\" + 0.015*\"russia\" + 0.013*\"zimbabw\" + 0.010*\"iraq\" + 0.009*\"iraqi\" + 0.009*\"trade\" + 0.008*\"greec\" + 0.008*\"crisi\"\n",
      "\n",
      "Score: 0.02000325173139572\t \n",
      "Topic: 0.026*\"nuclear\" + 0.021*\"iran\" + 0.013*\"pirat\" + 0.012*\"israel\" + 0.011*\"japan\" + 0.009*\"missil\" + 0.008*\"plant\" + 0.008*\"dubai\" + 0.008*\"fire\" + 0.008*\"pictur\"\n",
      "\n",
      "Score: 0.0200026948004961\t \n",
      "Topic: 0.015*\"human\" + 0.015*\"right\" + 0.010*\"wikileak\" + 0.009*\"crimin\" + 0.008*\"european\" + 0.008*\"assang\" + 0.007*\"countri\" + 0.007*\"australia\" + 0.007*\"say\" + 0.007*\"judg\"\n",
      "\n",
      "Score: 0.020002564415335655\t \n",
      "Topic: 0.014*\"elect\" + 0.013*\"russian\" + 0.011*\"putin\" + 0.011*\"presid\" + 0.011*\"say\" + 0.010*\"russia\" + 0.009*\"militari\" + 0.008*\"chavez\" + 0.008*\"vote\" + 0.008*\"parliament\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with unknown example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.41715794801712036\t Topic: 0.072*\"israel\" + 0.050*\"iran\" + 0.035*\"gaza\" + 0.030*\"protest\" + 0.030*\"isra\"\n",
      "Score: 0.38275665044784546\t Topic: 0.032*\"polic\" + 0.019*\"secret\" + 0.016*\"forc\" + 0.015*\"govern\" + 0.014*\"chines\"\n",
      "Score: 0.02502170018851757\t Topic: 0.045*\"world\" + 0.022*\"year\" + 0.018*\"time\" + 0.016*\"india\" + 0.013*\"minist\"\n",
      "Score: 0.025017810985445976\t Topic: 0.044*\"north\" + 0.043*\"korea\" + 0.040*\"south\" + 0.026*\"unit\" + 0.025*\"nation\"\n",
      "Score: 0.025016380473971367\t Topic: 0.020*\"germani\" + 0.015*\"dead\" + 0.015*\"german\" + 0.013*\"abus\" + 0.011*\"rise\"\n",
      "Score: 0.025013556703925133\t Topic: 0.068*\"china\" + 0.022*\"world\" + 0.019*\"parti\" + 0.012*\"govern\" + 0.012*\"worker\"\n",
      "Score: 0.025006653741002083\t Topic: 0.036*\"russian\" + 0.035*\"russia\" + 0.025*\"troop\" + 0.019*\"afghanistan\" + 0.018*\"syria\"\n",
      "Score: 0.025005050003528595\t Topic: 0.027*\"drug\" + 0.019*\"muslim\" + 0.019*\"mexico\" + 0.014*\"franc\" + 0.013*\"school\"\n",
      "Score: 0.025002136826515198\t Topic: 0.040*\"year\" + 0.031*\"death\" + 0.020*\"prison\" + 0.019*\"kill\" + 0.019*\"iraq\"\n",
      "Score: 0.02500211074948311\t Topic: 0.052*\"attack\" + 0.047*\"kill\" + 0.031*\"pakistan\" + 0.031*\"isra\" + 0.026*\"arrest\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
